name: AEGIS Data Pipeline

on:
  schedule:
    # Run every 5 minutes during market hours
    - cron: '*/5 * * * *'
  
  workflow_dispatch:  # Manual trigger
  
  push:
    branches:
      - main
    paths:
      - 'src/core/**'
      - 'config/**'
      - '.github/workflows/data_pipeline.yml'

env:
  PYTHON_VERSION: '3.11'
  DATA_BRANCH: 'data-storage'

jobs:
  update-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: |
          mkdir -p data/raw data/processed data/cache logs
      
      - name: Fetch and update data
        env:
          # Optional: Add API keys if needed for private endpoints
          BINANCE_API_KEY: ${{ secrets.BINANCE_API_KEY }}
          BINANCE_SECRET: ${{ secrets.BINANCE_SECRET }}
        run: |
          python -c "
          from src.core.data_fetcher import update_all_data
          import logging
          logging.basicConfig(level=logging.INFO)
          
          try:
              data = update_all_data()
              print(f'Successfully updated {len(data)} assets')
              for symbol, timeframes in data.items():
                  print(f'  {symbol}: {list(timeframes.keys())}')
          except Exception as e:
              print(f'Error: {e}')
              raise
          "
      
      - name: Validate data quality
        run: |
          python -c "
          from src.core.data_fetcher import DataPipeline
          import pandas as pd
          
          pipeline = DataPipeline()
          summary = pipeline.get_data_summary()
          
          if not summary.empty:
              print('Data Summary:')
              print(summary.to_string())
              
              # Check for stale data
              latest = summary['end_date'].max()
              from datetime import datetime, timedelta
              if datetime.now() - latest > timedelta(hours=1):
                  print('WARNING: Data may be stale!')
          else:
              print('No data found')
          "
      
      - name: Upload data artifacts
        uses: actions/upload-artifact@v3
        with:
          name: market-data
          path: |
            data/raw/*.parquet
            data/processed/*.parquet
          retention-days: 7
      
      - name: Commit data to separate branch
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Fetch or create data branch
          git fetch origin
          if git show-ref --verify --quiet refs/remotes/origin/${{ env.DATA_BRANCH }}; then
            git checkout ${{ env.DATA_BRANCH }}
          else
            git checkout --orphan ${{ env.DATA_BRANCH }}
            git rm -rf .
          fi
          
          # Add data files
          git add data/
          git add logs/ || true
          
          # Commit if there are changes
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update market data - $(date -u +%Y-%m-%d_%H:%M:%S)"
            git push origin ${{ env.DATA_BRANCH }}
          fi
      
      - name: Report status to Discord
        if: always()
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          STATUS="${{ job.status }}"
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          
          if [ "$STATUS" == "success" ]; then
            COLOR="3066993"
            TITLE="✅ Data Pipeline Success"
          else
            COLOR="15158332"
            TITLE="❌ Data Pipeline Failed"
          fi
          
          curl -H "Content-Type: application/json" \
            -d "{
              \"embeds\": [{
                \"title\": \"$TITLE\",
                \"color\": $COLOR,
                \"timestamp\": \"$TIMESTAMP\",
                \"fields\": [
                  {\"name\": \"Run ID\", \"value\": \"${{ github.run_id }}\", \"inline\": true},
                  {\"name\": \"Branch\", \"value\": \"${{ github.ref }}\", \"inline\": true}
                ]
              }]
            }" \
            $DISCORD_WEBHOOK || true
