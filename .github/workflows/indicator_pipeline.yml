name: AEGIS Indicator Pipeline

on:
  workflow_run:
    workflows: ["AEGIS Data Pipeline"]
    types:
      - completed
    branches:
      - main
  
  workflow_dispatch:
  
  schedule:
    - cron: '*/5 * * * *'

jobs:
  calculate-indicators:
    runs-on: ubuntu-latest
    # Only run if data pipeline succeeded or triggered manually/scheduled
    if: ${{ github.event_name != 'workflow_run' || github.event.workflow_run.conclusion == 'success' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: |
          mkdir -p data/raw data/processed data/cache logs
      
      - name: Download data artifacts from triggering workflow
        if: ${{ github.event_name == 'workflow_run' }}
        uses: actions/download-artifact@v4
        with:
          name: market-data
          path: data/raw
          github-token: ${{ secrets.GITHUB_TOKEN }}
          # Use run-id to get artifact from triggering workflow
          run-id: ${{ github.event.workflow_run.id }}
      
      - name: Download data artifacts from current workflow
        if: ${{ github.event_name != 'workflow_run' }}
        uses: actions/download-artifact@v4
        with:
          name: market-data
          path: data/raw
          github-token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Fallback to data branch if no artifact
        run: |
          # If artifact download failed or empty, try to get from data-storage branch
          if [ ! "$(ls -A data/raw/*.parquet 2>/dev/null)" ]; then
            echo "No artifact found, fetching from data-storage branch..."
            git fetch origin data-storage:data-storage || true
            git checkout data-storage -- data/raw/ || true
            git checkout main  # Return to main
          fi
      
      - name: Calculate indicators and confluence
        run: |
          python -c "
          import sys
          import os
          sys.path.insert(0, 'src')
          
          from core.data_fetcher import DataPipeline
          from indicators import IndicatorOrchestrator
          from analysis.market_regime import RegimeDetector
          from analysis.confluence import ConfluenceEngine
          from analysis.correlation import MultiTimeframeAnalyzer
          import pandas as pd
          import json
          from datetime import datetime
          
          # Initialize components
          pipeline = DataPipeline()
          orchestrator = IndicatorOrchestrator()
          regime_detector = RegimeDetector()
          confluence_engine = ConfluenceEngine()
          tf_analyzer = MultiTimeframeAnalyzer()
          
          # Get asset list
          import yaml
          with open('config/assets.yaml') as f:
              assets = yaml.safe_load(f)
          
          signals = []
          
          for tier in ['tier_1', 'tier_2']:
              for asset in assets['assets'].get(tier, []):
                  symbol = asset['symbol']
                  try:
                      print(f'Processing {symbol}...')
                      
                      # Fetch multi-timeframe data
                      tf_data = {}
                      for tf in ['1h', '4h', '1d']:
                          df = pipeline.fetch_complete_data(symbol, tf, update_only=True)
                          if df is not None and len(df) > 50:
                              # Calculate indicators
                              df = orchestrator.calculate_all(df)
                              # Add regime features
                              df = regime_detector.calculate_regime_features(df)
                              tf_data[tf] = df
                      
                      if not tf_data:
                          continue
                      
                      # Calculate confluence for primary timeframe
                      primary_df = tf_data['1h']
                      confluence = confluence_engine.calculate_confluence(primary_df)
                      summary = confluence_engine.get_signal_summary(confluence)
                      
                      # Multi-timeframe analysis
                      tf_signal = tf_analyzer.generate_multi_timeframe_signal(tf_data)
                      
                      signal_record = {
                          'timestamp': datetime.utcnow().isoformat(),
                          'symbol': symbol,
                          'price': float(primary_df['close'].iloc[-1]),
                          'timeframe_confluence': summary,
                          'multi_timeframe': tf_signal,
                          'indicators': orchestrator.get_indicator_summary(primary_df)
                      }
                      
                      signals.append(signal_record)
                      print(f'{symbol}: {summary[\"direction\"]} (Score: {summary[\"score\"]}, Confidence: {summary[\"confidence\"]})')
                      
                  except Exception as e:
                      print(f'Error processing {symbol}: {e}')
                      continue
          
          # Save signals
          os.makedirs('data/processed', exist_ok=True)
          with open('data/processed/latest_signals.json', 'w') as f:
              json.dump(signals, f, indent=2, default=str)
          
          print(f'Generated {len(signals)} signals')
          "
      
      - name: Upload processed signals
        uses: actions/upload-artifact@v4
        with:
          name: processed-signals  # Different name for output
          path: data/processed/latest_signals.json
          retention-days: 1
          overwrite: true
      
      - name: Commit processed data to branch
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git fetch origin data-storage
          git checkout data-storage || git checkout --orphan data-storage
          
          mkdir -p processed
          cp data/processed/latest_signals.json processed/ || true
          
          git add processed/
          git commit -m "Update processed signals - $(date -u)" || echo "No changes"
          git push origin data-storage || echo "Nothing to push"
      
      - name: Report status to Discord
        if: always()
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          STATUS="${{ job.status }}"
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          
          if [ "$STATUS" == "success" ]; then
            COLOR="3066993"
            TITLE="✅ Indicator Pipeline Success"
          else
            COLOR="15158332"
            TITLE="❌ Indicator Pipeline Failed"
          fi
          
          curl -H "Content-Type: application/json" \
            -d "{
              \"embeds\": [{
                \"title\": \"$TITLE\",
                \"color\": $COLOR,
                \"timestamp\": \"$TIMESTAMP\",
                \"fields\": [
                  {\"name\": \"Run ID\", \"value\": \"${{ github.run_id }}\", \"inline\": true}
                ]
              }]
            }" \
            $DISCORD_WEBHOOK || true
