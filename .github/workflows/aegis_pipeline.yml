name: AEGIS Complete Pipeline

on:
  schedule:
    # Run every 5 minutes
    - cron: '*/5 * * * *'
  
  workflow_dispatch:  # Manual trigger
  
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'config/**'
      - '.github/workflows/aegis_pipeline.yml'

env:
  PYTHON_VERSION: '3.11'
  DATA_BRANCH: 'data-storage'

jobs:
  # Job 1: Fetch market data
  fetch-data:
    runs-on: ubuntu-latest
    
    outputs:
      data_available: ${{ steps.check_data.outputs.data_available }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: mkdir -p data/raw data/processed data/cache logs signals
      
      - name: Restore data from branch
        run: |
          # Try to restore existing data from data-storage branch
          git fetch origin data-storage || true
          if git show-ref --verify --quiet refs/remotes/origin/data-storage; then
            git checkout data-storage -- data/raw/ 2>/dev/null || true
            git checkout data-storage -- data/processed/ 2>/dev/null || true
            git checkout main 2>/dev/null || true
          fi
          ls -la data/raw/ 2>/dev/null || echo "No existing data"
      
      - name: Fetch market data
        id: fetch_data
        env:
          BINANCE_API_KEY: ${{ secrets.BINANCE_API_KEY }}
          BINANCE_SECRET: ${{ secrets.BINANCE_SECRET }}
          HTTP_PROXY: ${{ secrets.HTTP_PROXY }}
          HTTPS_PROXY: ${{ secrets.HTTPS_PROXY }}
        run: |
          python -c "
          from src.core.data_fetcher import update_all_data
          import logging
          import os
          import pandas as pd
          
          logging.basicConfig(level=logging.INFO)
          
          try:
              data = update_all_data()
              print(f'Successfully updated {len(data)} assets')
              
              # Check if we have data
              has_data = len(data) > 0
              
              # Also check for cached data
              if not has_data:
                  try:
                      from src.core.data_fetcher import DataPipeline
                      pipeline = DataPipeline()
                      summary = pipeline.get_data_summary()
                      has_data = not summary.empty
                  except:
                      pass
              
              # Set output
              with open(os.environ.get('GITHUB_OUTPUT', '/tmp/github_output'), 'a') as f:
                  f.write(f'data_available={str(has_data).lower()}\n')
                  f.write(f'assets_count={len(data)}\n')
                  
          except Exception as e:
              print(f'Error in data fetch: {e}')
              # Check if we have cached data
              try:
                  from src.core.data_fetcher import DataPipeline
                  pipeline = DataPipeline()
                  summary = pipeline.get_data_summary()
                  has_data = not summary.empty
                  print(f'Using cached data: {len(summary)} files')
              except Exception as e2:
                  print(f'No cached data available: {e2}')
                  has_data = False
              
              with open(os.environ.get('GITHUB_OUTPUT', '/tmp/github_output'), 'a') as f:
                  f.write(f'data_available={str(has_data).lower()}\n')
                  f.write(f'fetch_error={str(e)}\n')
          "
      
      - name: Check data availability
        id: check_data
        run: |
          # Read the output from previous step
          if [ -f /tmp/github_output ]; then
            cat /tmp/github_output >> $GITHUB_OUTPUT
          fi
          
          # Default to checking if files exist
          if [ -n "$(ls -A data/raw/*.parquet 2>/dev/null)" ]; then
            echo "data_available=true" >> $GITHUB_OUTPUT
            echo "Found existing data files"
          else
            echo "data_available=false" >> $GITHUB_OUTPUT
            echo "No data files found"
          fi
      
      - name: Upload data artifacts
        uses: actions/upload-artifact@v4
        with:
          name: market-data-${{ github.run_id }}
          path: |
            data/raw/*.parquet
          retention-days: 1
          overwrite: true
      
      - name: Commit data to branch
        if: always()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git fetch origin
          
          # Create or checkout data-storage branch
          if git show-ref --verify --quiet refs/remotes/origin/data-storage; then
            git checkout data-storage
            git rm -rf . 2>/dev/null || true
          else
            git checkout --orphan data-storage
          fi
          
          # Copy data from main branch run
          git checkout main -- data/raw/ 2>/dev/null || true
          git checkout main -- data/processed/ 2>/dev/null || true
          
          # Add all data
          git add data/ logs/ 2>/dev/null || true
          
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update market data - $(date -u +%Y-%m-%d_%H:%M:%S) [run: ${{ github.run_id }}]"
            git push origin data-storage
          fi
          
          # Return to main
          git checkout main || true

  # Job 2: Calculate indicators (depends on fetch-data)
  calculate-indicators:
    runs-on: ubuntu-latest
    needs: fetch-data
    if: ${{ needs.fetch-data.outputs.data_available == 'true' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: mkdir -p data/raw data/processed data/cache logs
      
      - name: Download data from previous job
        uses: actions/download-artifact@v4
        with:
          name: market-data-${{ github.run_id }}
          path: data/raw
      
      - name: Calculate indicators and generate signals
        run: |
          python -c "
          import sys
          import os
          sys.path.insert(0, 'src')
          
          from core.data_fetcher import DataPipeline
          from core.signal_generator import SignalGenerator
          from notifications.formatter import SignalFormatter
          import json
          from datetime import datetime
          
          # Initialize
          pipeline = DataPipeline()
          generator = SignalGenerator(risk_level='moderate', use_ml=True)
          formatter = SignalFormatter()
          
          # Fetch all data with indicators
          all_data = pipeline.fetch_all_assets()
          
          if not all_data:
              print('No data available')
              exit(1)
          
          # Generate signals
          signals = generator.generate_all_signals(all_data, account_balance=10000)
          
          print(f'Generated {len(signals)} signals')
          
          # Save signals
          os.makedirs('data/processed', exist_ok=True)
          signals_data = []
          for s in signals:
              signals_data.append({
                  'timestamp': s.timestamp.isoformat() if hasattr(s, 'timestamp') else datetime.utcnow().isoformat(),
                  'symbol': s.symbol if hasattr(s, 'symbol') else 'unknown',
                  'direction': s.direction if hasattr(s, 'direction') else 'neutral',
                  'confidence': s.confidence if hasattr(s, 'confidence') else 'low',
                  'confidence_score': s.confidence_score if hasattr(s, 'confidence_score') else 0,
                  'entry_price': s.entry_price if hasattr(s, 'entry_price') else 0,
                  'stop_loss': s.stop_loss if hasattr(s, 'stop_loss') else 0,
                  'take_profit': s.take_profit if hasattr(s, 'take_profit') else 0,
                  'position_size': s.position_size if hasattr(s, 'position_size') else {},
                  'timeframe_confluence': s.timeframe_confluence if hasattr(s, 'timeframe_confluence') else {},
                  'risk_metrics': s.risk_metrics if hasattr(s, 'risk_metrics') else {}
              })
          
          with open('data/processed/latest_signals.json', 'w') as f:
              json.dump(signals_data, f, indent=2, default=str)
          
          # Save individual signal files for history
          os.makedirs('signals', exist_ok=True)
          timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
          with open(f'signals/signals_{timestamp}.json', 'w') as f:
              json.dump(signals_data, f, indent=2, default=str)
          
          print(f'Saved {len(signals_data)} signals')
          "
      
      - name: Upload signals artifact
        uses: actions/upload-artifact@v4
        with:
          name: trading-signals-${{ github.run_id }}
          path: |
            data/processed/latest_signals.json
            signals/signals_*.json
          retention-days: 7
          overwrite: true
      
      - name: Send signals to Discord
        if: success()
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          python -c "
          import json
          import os
          import requests
          from datetime import datetime
          
          webhook = os.environ.get('DISCORD_WEBHOOK')
          if not webhook:
              print('No Discord webhook configured')
              exit(0)
          
          try:
              with open('data/processed/latest_signals.json') as f:
                  signals = json.load(f)
          except Exception as e:
              print(f'Error loading signals: {e}')
              signals = []
          
          if not signals:
              # Send heartbeat
              embed = {
                  'title': 'üíì AEGIS Heartbeat',
                  'description': 'Scan complete. No high-quality signals at this time.',
                  'color': 9807270,
                  'timestamp': datetime.utcnow().isoformat()
              }
              try:
                  requests.post(webhook, json={'embeds': [embed]})
              except:
                  pass
              print('Sent heartbeat')
              exit(0)
          
          # Send summary
          longs = sum(1 for s in signals if s.get('direction') == 'long')
          shorts = sum(1 for s in signals if s.get('direction') == 'short')
          high_conf = sum(1 for s in signals if s.get('confidence') in ['high', 'very_high'])
          
          summary_embed = {
              'title': 'üõ°Ô∏è AEGIS Signal Alert',
              'description': f'Found {len(signals)} tradeable signals',
              'color': 3447003,
              'timestamp': datetime.utcnow().isoformat(),
              'fields': [
                  {'name': 'üü¢ Long', 'value': str(longs), 'inline': True},
                  {'name': 'üî¥ Short', 'value': str(shorts), 'inline': True},
                  {'name': 'üî• High Confidence', 'value': str(high_conf), 'inline': True}
              ]
          }
          
          try:
              requests.post(webhook, json={'embeds': [summary_embed]})
              print(f'Sent summary: {len(signals)} signals')
          except Exception as e:
              print(f'Failed to send summary: {e}')
          
          # Send high-confidence individual signals
          for signal in signals:
              if signal.get('confidence') not in ['high', 'very_high']:
                  continue
              
              try:
                  direction = signal.get('direction', 'neutral')
                  color = 3066993 if direction == 'long' else 15158332
                  emoji = 'üü¢' if direction == 'long' else 'üî¥'
                  
                  embed = {
                      'title': f\\\"{emoji} {signal.get('symbol', 'Unknown')} Signal\\\",\                      'color': color,
                      'timestamp': signal.get('timestamp', datetime.utcnow().isoformat()),
                      'fields': [
                          {'name': 'Direction', 'value': direction.upper(), 'inline': True},
                          {'name': 'Confidence', 'value': f\\\"{signal.get('confidence_score', 0):.0%}\\\", 'inline': True},
                          {'name': 'Entry', 'value': f\\\"${signal.get('entry_price', 0):,.2f}\\\", 'inline': True},
                          {'name': 'Stop', 'value': f\\\"${signal.get('stop_loss', 0):,.2f}\\\", 'inline': True},
                          {'name': 'Target', 'value': f\\\"${signal.get('take_profit', 0):,.2f}\\\", 'inline': True}
                      ]
                  }
                  
                  requests.post(webhook, json={'embeds': [embed]})
                  print(f\\\"Sent: {signal.get('symbol')}\\\")
              except Exception as e:
                  print(f'Failed to send signal: {e}')
          "
      
      - name: Commit signals to branch
        if: always()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git fetch origin
          
          # Create or checkout data-storage branch
          if git show-ref --verify --quiet refs/remotes/origin/data-storage; then
            git checkout data-storage
          else
            git checkout --orphan data-storage
            git rm -rf . 2>/dev/null || true
          fi
          
          # Copy from main
          git checkout main -- data/processed/ 2>/dev/null || true
          git checkout main -- signals/ 2>/dev/null || true
          
          git add data/processed/ signals/ 2>/dev/null || true
          
          if git diff --cached --quiet; then
            echo "No changes"
          else
            git commit -m "Update signals - $(date -u) [run: ${{ github.run_id }}]"
            git push origin data-storage
          fi
          
          git checkout main || true

  # Job 3: Report status (always runs)
  report-status:
    runs-on: ubuntu-latest
    needs: [fetch-data, calculate-indicators]
    if: always()
    
    steps:
      - name: Report pipeline status
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          DATA_STATUS="${{ needs.fetch-data.result }}"
          INDICATOR_STATUS="${{ needs.calculate-indicators.result }}"
          
          if [ "$DATA_STATUS" == "success" ] && [ "$INDICATOR_STATUS" == "success" ]; then
            COLOR="3066993"
            TITLE="‚úÖ AEGIS Pipeline Complete"
            MESSAGE="Data fetched and signals generated successfully"
          elif [ "$DATA_STATUS" == "success" ]; then
            COLOR="16776960"
            TITLE="‚ö†Ô∏è AEGIS Partial Success"
            MESSAGE="Data fetched but signal generation failed"
          else
            COLOR="15158332"
            TITLE="‚ùå AEGIS Pipeline Failed"
            MESSAGE="Data fetch failed - check logs"
          fi
          
          curl -H "Content-Type: application/json" \
            -d "{
              \"embeds\": [{
                \"title\": \"$TITLE\",
                \"description\": \"$MESSAGE\",
                \"color\": $COLOR,
                \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
                \"fields\": [
                  {\"name\": \"Data Fetch\", \"value\": \"$DATA_STATUS\", \"inline\": true},
                  {\"name\": \"Signal Gen\", \"value\": \"$INDICATOR_STATUS\", \"inline\": true},
                  {\"name\": \"Run ID\", \"value\": \"${{ github.run_id }}\", \"inline\": true}
                ]
              }]
            }" \
            $DISCORD_WEBHOOK || true
