name: AEGIS Complete Pipeline

on:
  schedule:
    - cron: '*/5 * * * *'
  
  workflow_dispatch:
  
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'config/**'
      - '.github/workflows/aegis_pipeline.yml'

permissions:
  contents: write
  actions: read

env:
  PYTHON_VERSION: '3.11'
  ARTIFACT_NAME: market-data

jobs:
  # Job 1: Fetch market data
  fetch-data:
    runs-on: ubuntu-latest
    
    outputs:
      data_available: ${{ steps.check_data.outputs.data_available }}
      data_count: ${{ steps.check_data.outputs.data_count }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: mkdir -p data/raw data/processed data/cache logs signals
      
      - name: Restore data from branch
        run: |
          git fetch origin data-storage 2>/dev/null || echo "No data-storage branch"
          if git show-ref --verify --quiet refs/remotes/origin/data-storage 2>/dev/null; then
            git checkout origin/data-storage -- data/raw/ 2>/dev/null || true
            git checkout origin/data-storage -- data/processed/ 2>/dev/null || true
          fi
          ls -la data/raw/ 2>/dev/null || echo "No existing data"
      
      - name: Fetch market data
        id: fetch_step
        env:
          BINANCE_API_KEY: ${{ secrets.BINANCE_API_KEY }}
          BINANCE_SECRET: ${{ secrets.BINANCE_SECRET }}
          HTTP_PROXY: ${{ secrets.HTTP_PROXY }}
          HTTPS_PROXY: ${{ secrets.HTTPS_PROXY }}
        run: |
          python -c "
          import sys
          sys.path.insert(0, 'src')
          from core.data_fetcher import update_all_data
          import logging
          import os
          from pathlib import Path
          
          logging.basicConfig(level=logging.INFO)
          
          data_count = 0
          has_data = False
          
          try:
              data = update_all_data()
              data_count = len(data) if data else 0
              has_data = data_count > 0
              print(f'Success: {data_count} assets updated')
          except Exception as e:
              print(f'Fetch error: {e}')
              # Check for cached data
              try:
                  parquet_files = list(Path('data/raw').glob('*.parquet'))
                  has_data = len(parquet_files) > 0
                  data_count = len(parquet_files)
                  print(f'Using cache: {data_count} files')
              except Exception as e2:
                  print(f'Cache check failed: {e2}')
                  has_data = False
          
          # Write to GITHUB_OUTPUT
          output_file = os.environ.get('GITHUB_OUTPUT')
          if output_file:
              with open(output_file, 'a') as f:
                  f.write(f'data_available={str(has_data).lower()}\n')
                  f.write(f'data_count={data_count}\n')
          
          print(f'Output: data_available={has_data}, count={data_count}')
          "
      
      - name: Check data availability
        id: check_data
        if: always()  # Always run to ensure outputs are set
        run: |
          # Count actual files
          FILE_COUNT=$(ls data/raw/*.parquet 2>/dev/null | wc -l)
          echo "Found $FILE_COUNT parquet files"
          
          if [ "$FILE_COUNT" -gt 0 ]; then
            echo "data_available=true" >> $GITHUB_OUTPUT
            echo "data_count=$FILE_COUNT" >> $GITHUB_OUTPUT
            echo "Data available: true"
          else
            echo "data_available=false" >> $GITHUB_OUTPUT
            echo "data_count=0" >> $GITHUB_OUTPUT
            echo "Data available: false"
          fi
      
      - name: Upload data artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.ARTIFACT_NAME }}
          path: data/raw/*.parquet
          retention-days: 1
          overwrite: true
      
      - name: Commit data to branch
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git fetch origin
          
          if git show-ref --verify --quiet refs/remotes/origin/data-storage; then
            git checkout data-storage
            git rm -rf . 2>/dev/null || true
          else
            git checkout --orphan data-storage
          fi
          
          git checkout main -- data/raw/ 2>/dev/null || true
          git checkout main -- data/processed/ 2>/dev/null || true
          
          git add data/ 2>/dev/null || true
          
          if ! git diff --cached --quiet; then
            git commit -m "Update data - $(date -u) [run: ${{ github.run_id }}]"
            git push "https://${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git" data-storage || true
          fi
          
          git checkout main || true

  # Job 2: Calculate indicators - Always runs when fetch-data succeeds
  calculate-indicators:
    runs-on: ubuntu-latest
    needs: fetch-data
    # FIXED: Always run when fetch-data succeeds, handle data absence internally
    if: ${{ needs.fetch-data.result == 'success' }}
    
    steps:
      - name: Debug - Check inputs
        run: |
          echo "Data available: '${{ needs.fetch-data.outputs.data_available }}'"
          echo "Data count: ${{ needs.fetch-data.outputs.data_count }}"
          echo "Fetch result: ${{ needs.fetch-data.result }}"
      
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: mkdir -p data/raw data/processed data/cache logs
      
      - name: Download data from artifact
        id: download_artifact
        continue-on-error: true  # Don't fail if artifact missing
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.ARTIFACT_NAME }}
          path: data/raw
      
      - name: Fallback to data branch
        if: steps.download_artifact.outcome == 'failure'
        run: |
          echo "Artifact download failed, trying data branch..."
          git fetch origin data-storage 2>/dev/null || true
          git checkout origin/data-storage -- data/raw/ 2>/dev/null || true
      
      - name: Verify data and set status
        id: verify_data
        run: |
          FILE_COUNT=$(ls data/raw/*.parquet 2>/dev/null | wc -l)
          echo "Total parquet files: $FILE_COUNT"
          echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          
          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "No data files available - will generate empty signals"
            echo "has_data=false" >> $GITHUB_OUTPUT
          else
            echo "Data available - proceeding with signal generation"
            echo "has_data=true" >> $GITHUB_OUTPUT
            ls -lh data/raw/*.parquet | head -5
          fi
      
      - name: Generate signals
        id: generate
        run: |
          python -c "
          import sys
          sys.path.insert(0, 'src')
          
          import json
          import os
          import traceback
          from datetime import datetime
          
          signals_data = []
          has_data = os.environ.get('HAS_DATA', 'false') == 'true'
          
          if not has_data:
              print('No data available - generating empty signals file')
          else:
              try:
                  from core.data_fetcher import DataPipeline
                  from core.signal_generator import SignalGenerator
                  
                  print('Initializing pipeline...')
                  pipeline = DataPipeline()
                  
                  print('Fetching all assets...')
                  all_data = pipeline.fetch_all_assets()
                  
                  if not all_data:
                      print('WARNING: No data returned from pipeline')
                  else:
                      print(f'Processing {len(all_data)} assets...')
                      
                      # Initialize generator
                      try:
                          generator = SignalGenerator(risk_level='moderate', use_ml=False)
                      except Exception as e:
                          print(f'Generator init error: {e}')
                          generator = None
                      
                      if generator and all_data:
                          try:
                              signals = generator.generate_all_signals(all_data, 10000)
                              print(f'Generated {len(signals)} signals')
                              
                              # Convert signals to JSON-serializable format
                              for s in signals:
                                  try:
                                      signal_dict = {
                                          'timestamp': datetime.utcnow().isoformat(),
                                          'symbol': getattr(s, 'symbol', 'unknown'),
                                          'direction': getattr(s, 'direction', 'neutral'),
                                          'confidence': getattr(s, 'confidence', 'low'),
                                          'confidence_score': float(getattr(s, 'confidence_score', 0)),
                                          'entry_price': float(getattr(s, 'entry_price', 0)),
                                          'stop_loss': float(getattr(s, 'stop_loss', 0)),
                                          'take_profit': float(getattr(s, 'take_profit', 0))
                                      }
                                      signals_data.append(signal_dict)
                                  except Exception as e:
                                      print(f'Error converting signal: {e}')
                                      continue
                          except Exception as e:
                              print(f'Signal generation error: {e}')
                              traceback.print_exc()
              
              except Exception as e:
                  print(f'Pipeline error: {e}')
                  traceback.print_exc()
          
          # Ensure output directory exists
          os.makedirs('data/processed', exist_ok=True)
          os.makedirs('signals', exist_ok=True)
          
          # Write signals file (empty or populated)
          output_path = 'data/processed/latest_signals.json'
          with open(output_path, 'w') as f:
              json.dump(signals_data, f, indent=2, default=str)
          
          print(f'Saved {len(signals_data)} signals to {output_path}')
          
          # Also save timestamped copy
          timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
          timestamp_path = f'signals/signals_{timestamp}.json'
          with open(timestamp_path, 'w') as f:
              json.dump(signals_data, f, indent=2, default=str)
          
          # Write signal count for next steps
          with open(os.environ.get('GITHUB_OUTPUT'), 'a') as f:
              f.write(f'signal_count={len(signals_data)}\n')
              f.write(f'has_data={str(has_data).lower()}\n')
          
          print('Signal generation completed')
          "
        env:
          HAS_DATA: ${{ steps.verify_data.outputs.has_data }}
      
      - name: Verify signals output
        run: |
          if [ -f "data/processed/latest_signals.json" ]; then
            echo "âœ“ Signals file created successfully"
            SIGNAL_COUNT=$(python -c "import json; print(len(json.load(open('data/processed/latest_signals.json'))))")
            echo "Signal count: $SIGNAL_COUNT"
          else
            echo "ERROR: Signals file not created"
            exit 1
          fi
      
      - name: Upload signals
        uses: actions/upload-artifact@v4
        with:
          name: trading-signals
          path: |
            data/processed/latest_signals.json
            signals/signals_*.json
          retention-days: 7
          overwrite: true
      
      - name: Send to Discord
        if: success()
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          python -c "
          import json
          import requests
          import os
          from datetime import datetime
          
          webhook = os.environ.get('DISCORD_WEBHOOK', '')
          if not webhook:
              print('No Discord webhook configured')
              exit(0)
          
          try:
              with open('data/processed/latest_signals.json') as f:
                  signals = json.load(f)
          except Exception as e:
              print(f'Error loading signals: {e}')
              signals = []
          
          # Always send heartbeat
          if not signals:
              embed = {
                  'title': 'ðŸ’“ AEGIS Heartbeat',
                  'description': 'No trading signals generated (no data or no opportunities)',
                  'color': 9807270,
                  'timestamp': datetime.utcnow().isoformat(),
                  'footer': {'text': '5-min cycle completed'}
              }
              try:
                  requests.post(webhook, json={'embeds': [embed]})
              except Exception as e:
                  print(f'Discord error: {e}')
              exit(0)
          
          # Summary for when we have signals
          longs = sum(1 for s in signals if s.get('direction') == 'long')
          shorts = sum(1 for s in signals if s.get('direction') == 'short')
          high_conf = sum(1 for s in signals if s.get('confidence') in ['high', 'very_high'])
          
          summary = {
              'title': f'ðŸ›¡ï¸ {len(signals)} Signals Generated',
              'color': 3447003,
              'fields': [
                  {'name': 'ðŸŸ¢ Long', 'value': str(longs), 'inline': True},
                  {'name': 'ðŸ”´ Short', 'value': str(shorts), 'inline': True},
                  {'name': 'â­ High Conf', 'value': str(high_conf), 'inline': True},
                  {'name': 'â±ï¸ Time', 'value': datetime.utcnow().strftime('%H:%M UTC'), 'inline': True}
              ],
              'timestamp': datetime.utcnow().isoformat()
          }
          
          try:
              requests.post(webhook, json={'embeds': [summary]})
          except Exception as e:
              print(f'Discord summary error: {e}')
          
          # Individual high-confidence signals
          for s in signals:
              if s.get('confidence') in ['high', 'very_high']:
                  try:
                      color = 3066993 if s.get('direction') == 'long' else 15158332
                      embed = {
                          'title': f\\\"{s.get('symbol', 'Unknown')} {s.get('direction', 'neutral').upper()}\\\",\                          'color': color,
                          'fields': [
                              {'name': 'Confidence', 'value': f\\\"{s.get('confidence_score', 0):.1%}\\\", 'inline': True},
                              {'name': 'Entry', 'value': f\\\"{s.get('entry_price', 0):.4f}\\\", 'inline': True},
                              {'name': 'SL/TP', 'value': f\\\"{s.get('stop_loss', 0):.4f} / {s.get('take_profit', 0):.4f}\\\", 'inline': True}
                          ]
                      }
                      requests.post(webhook, json={'embeds': [embed]})
                  except Exception as e:
                      print(f'Error sending signal: {e}')
          "
      
      - name: Commit signals
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git fetch origin
          
          if git show-ref --verify --quiet refs/remotes/origin/data-storage; then
            git checkout data-storage
          else
            git checkout --orphan data-storage
            git rm -rf . 2>/dev/null || true
          fi
          
          git checkout main -- data/processed/ 2>/dev/null || true
          git checkout main -- signals/ 2>/dev/null || true
          
          git add data/processed/ signals/ 2>/dev/null || true
          
          if ! git diff --cached --quiet; then
            git commit -m "Signals - $(date -u) [${{ github.run_id }}]"
            git push "https://${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git" data-storage || true
          fi
          
          git checkout main || true

  # Job 3: Report status
  report-status:
    runs-on: ubuntu-latest
    needs: [fetch-data, calculate-indicators]
    if: always()
    
    steps:
      - name: Report status
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          DATA="${{ needs.fetch-data.result }}"
          INDICATOR="${{ needs.calculate-indicators.result }}"
          DATA_AVAILABLE="${{ needs.fetch-data.outputs.data_available }}"
          DATA_COUNT="${{ needs.fetch-data.outputs.data_count }}"
          
          # Determine status color and title
          if [ "$DATA" == "success" ] && [ "$INDICATOR" == "success" ]; then
            if [ "$DATA_AVAILABLE" == "true" ]; then
              COLOR="3066993"
              TITLE="âœ… AEGIS Success"
              DESC="Pipeline completed with data"
            else
              COLOR="16776960"
              TITLE="âš ï¸ AEGIS No Data"
              DESC="Fetch succeeded but no data available"
            fi
          elif [ "$DATA" == "success" ]; then
            COLOR="15158332"
            TITLE="âŒ AEGIS Signal Gen Failed"
            DESC="Data fetch succeeded but signal generation failed"
          else
            COLOR="15158332"
            TITLE="âŒ AEGIS Data Fetch Failed"
            DESC="Could not retrieve market data"
          fi
          
          curl -H "Content-Type: application/json" \
            -d "{
              \"embeds\": [{
                \"title\": \"$TITLE\",
                \"description\": \"$DESC\",
                \"color\": $COLOR,
                \"fields\": [
                  {\"name\": \"Data Fetch\", \"value\": \"$DATA\", \"inline\": true},
                  {\"name\": \"Signal Gen\", \"value\": \"$INDICATOR\", \"inline\": true},
                  {\"name\": \"Data Available\", \"value\": \"$DATA_AVAILABLE ($DATA_COUNT files)\", \"inline\": true}
                ],
                \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
              }]
            }" \
            $DISCORD_WEBHOOK 2>/dev/null || echo "Discord webhook failed"
