name: AEGIS Complete Pipeline

on:
  schedule:
    - cron: '*/5 * * * *'
  
  workflow_dispatch:
  
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'config/**'
      - '.github/workflows/aegis_pipeline.yml'

permissions:
  contents: write
  actions: read

env:
  PYTHON_VERSION: '3.11'

jobs:
  # Job 1: Fetch market data
  fetch-data:
    runs-on: ubuntu-latest
    
    outputs:
      data_available: ${{ steps.check_data.outputs.data_available }}
      data_count: ${{ steps.check_data.outputs.data_count }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: mkdir -p data/raw data/processed data/cache logs signals
      
      - name: Restore data from branch
        run: |
          git fetch origin data-storage 2>/dev/null || echo "No data-storage branch"
          if git show-ref --verify --quiet refs/remotes/origin/data-storage 2>/dev/null; then
            git checkout origin/data-storage -- data/raw/ 2>/dev/null || true
            git checkout origin/data-storage -- data/processed/ 2>/dev/null || true
          fi
          ls -la data/raw/ 2>/dev/null || echo "No existing data"
      
      - name: Fetch market data
        id: fetch_step
        env:
          BINANCE_API_KEY: ${{ secrets.BINANCE_API_KEY }}
          BINANCE_SECRET: ${{ secrets.BINANCE_SECRET }}
          HTTP_PROXY: ${{ secrets.HTTP_PROXY }}
          HTTPS_PROXY: ${{ secrets.HTTPS_PROXY }}
        run: |
          python -c "
          from src.core.data_fetcher import update_all_data
          import logging
          import os
          import json
          
          logging.basicConfig(level=logging.INFO)
          
          data_count = 0
          has_data = False
          
          try:
              data = update_all_data()
              data_count = len(data)
              has_data = data_count > 0
              print(f'Success: {data_count} assets updated')
          except Exception as e:
              print(f'Fetch error: {e}')
              # Check for cached data
              try:
                  import pandas as pd
                  from pathlib import Path
                  parquet_files = list(Path('data/raw').glob('*.parquet'))
                  has_data = len(parquet_files) > 0
                  data_count = len(parquet_files)
                  print(f'Using cache: {data_count} files')
              except Exception as e2:
                  print(f'Cache check failed: {e2}')
                  has_data = False
          
          # Write to GITHUB_OUTPUT for step output
          output_file = os.environ.get('GITHUB_OUTPUT')
          if output_file:
              with open(output_file, 'a') as f:
                  f.write(f'data_available={str(has_data).lower()}\n')
                  f.write(f'data_count={data_count}\n')
          
          print(f'Output: data_available={has_data}, count={data_count}')
          "
      
      - name: Check data availability
        id: check_data
        run: |
          # Get from previous step output
          echo "data_available=${{ steps.fetch_step.outputs.data_available }}" >> $GITHUB_OUTPUT
          echo "data_count=${{ steps.fetch_step.outputs.data_count }}" >> $GITHUB_OUTPUT
          
          # Verify by listing files
          FILE_COUNT=$(ls data/raw/*.parquet 2>/dev/null | wc -l)
          echo "Found $FILE_COUNT parquet files"
          
          if [ "$FILE_COUNT" -gt 0 ]; then
            echo "data_available=true" >> $GITHUB_OUTPUT
            echo "data_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          else
            echo "data_available=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Upload data artifacts
        uses: actions/upload-artifact@v4
        with:
          name: market-data-${{ github.run_id }}
          path: data/raw/*.parquet
          retention-days: 1
          overwrite: true
      
      - name: Commit data to branch
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git fetch origin
          
          if git show-ref --verify --quiet refs/remotes/origin/data-storage; then
            git checkout data-storage
            git rm -rf . 2>/dev/null || true
          else
            git checkout --orphan data-storage
          fi
          
          git checkout main -- data/raw/ 2>/dev/null || true
          git checkout main -- data/processed/ 2>/dev/null || true
          
          git add data/ 2>/dev/null || true
          
          if ! git diff --cached --quiet; then
            git commit -m "Update data - $(date -u) [run: ${{ github.run_id }}]"
            git push "https://${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git" data-storage || true
          fi
          
          git checkout main || true

  # Job 2: Calculate indicators - DEPENDS ON fetch-data
  calculate-indicators:
    runs-on: ubuntu-latest
    needs: fetch-data
    # Run if data is available OR if this is manual/scheduled trigger
    if: ${{ needs.fetch-data.outputs.data_available == 'true' || github.event_name != 'workflow_run' }}
    
    steps:
      - name: Debug - Check inputs
        run: |
          echo "Data available: ${{ needs.fetch-data.outputs.data_available }}"
          echo "Data count: ${{ needs.fetch-data.outputs.data_count }}"
          echo "Event name: ${{ github.event_name }}"
      
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: mkdir -p data/raw data/processed data/cache logs
      
      - name: Download data from previous job
        uses: actions/download-artifact@v4
        with:
          name: market-data-${{ github.run_id }}
          path: data/raw
          continue-on-error: true
      
      - name: Fallback to data branch
        run: |
          if [ ! "$(ls -A data/raw/*.parquet 2>/dev/null)" ]; then
            echo "Fetching from branch..."
            git fetch origin data-storage 2>/dev/null || true
            git checkout origin/data-storage -- data/raw/ 2>/dev/null || true
          fi
          
          # Final check
          FILE_COUNT=$(ls data/raw/*.parquet 2>/dev/null | wc -l)
          echo "Total files: $FILE_COUNT"
          
          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "ERROR: No data files available"
            exit 1
          fi
      
      - name: Generate signals
        run: |
          python -c "
          import sys
          sys.path.insert(0, 'src')
          
          from core.data_fetcher import DataPipeline
          from core.signal_generator import SignalGenerator
          import json
          from datetime import datetime
          import os
          
          pipeline = DataPipeline()
          generator = SignalGenerator(risk_level='moderate', use_ml=False)
          
          all_data = pipeline.fetch_all_assets()
          
          if not all_data:
              print('No data')
              os.makedirs('data/processed', exist_ok=True)
              with open('data/processed/latest_signals.json', 'w') as f:
                  json.dump([], f)
          else:
              signals = generator.generate_all_signals(all_data, 10000)
              print(f'Generated {len(signals)} signals')
              
              signals_data = []
              for s in signals:
                  signals_data.append({
                      'timestamp': s.timestamp.isoformat() if hasattr(s, 'timestamp') else datetime.utcnow().isoformat(),
                      'symbol': s.symbol if hasattr(s, 'symbol') else 'unknown',
                      'direction': s.direction if hasattr(s, 'direction') else 'neutral',
                      'confidence': s.confidence if hasattr(s, 'confidence') else 'low',
                      'confidence_score': s.confidence_score if hasattr(s, 'confidence_score') else 0,
                      'entry_price': s.entry_price if hasattr(s, 'entry_price') else 0,
                      'stop_loss': s.stop_loss if hasattr(s, 'stop_loss') else 0,
                      'take_profit': s.take_profit if hasattr(s, 'take_profit') else 0
                  })
              
              os.makedirs('data/processed', exist_ok=True)
              with open('data/processed/latest_signals.json', 'w') as f:
                  json.dump(signals_data, f, indent=2, default=str)
              
              os.makedirs('signals', exist_ok=True)
              timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
              with open(f'signals/signals_{timestamp}.json', 'w') as f:
                  json.dump(signals_data, f, indent=2, default=str)
          "
      
      - name: Upload signals
        uses: actions/upload-artifact@v4
        with:
          name: trading-signals-${{ github.run_id }}
          path: |
            data/processed/latest_signals.json
            signals/signals_*.json
          retention-days: 7
          overwrite: true
      
      - name: Send to Discord
        if: success()
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          python -c "
          import json
          import requests
          from datetime import datetime
          
          webhook = '${{ secrets.DISCORD_WEBHOOK }}'
          if not webhook:
              exit(0)
          
          try:
              with open('data/processed/latest_signals.json') as f:
                  signals = json.load(f)
          except:
              signals = []
          
          if not signals:
              embed = {
                  'title': 'ðŸ’“ AEGIS Heartbeat',
                  'description': 'No signals',
                  'color': 9807270,
                  'timestamp': datetime.utcnow().isoformat()
              }
              requests.post(webhook, json={'embeds': [embed]})
              exit(0)
          
          # Summary
          longs = sum(1 for s in signals if s.get('direction') == 'long')
          shorts = sum(1 for s in signals if s.get('direction') == 'short')
          
          summary = {
              'title': f'ðŸ›¡ï¸ {len(signals)} Signals',
              'color': 3447003,
              'fields': [
                  {'name': 'ðŸŸ¢ Long', 'value': str(longs), 'inline': True},
                  {'name': 'ðŸ”´ Short', 'value': str(shorts), 'inline': True}
              ],
              'timestamp': datetime.utcnow().isoformat()
          }
          requests.post(webhook, json={'embeds': [summary]})
          
          # Individual signals
          for s in signals:
              if s.get('confidence') in ['high', 'very_high']:
                  color = 3066993 if s.get('direction') == 'long' else 15158332
                  embed = {
                      'title': f\\\"{s.get('symbol')} {s.get('direction').upper()}\\\",\                      'color': color,
                      'fields': [
                          {'name': 'Confidence', 'value': str(s.get('confidence_score', 0)), 'inline': True},
                          {'name': 'Entry', 'value': str(s.get('entry_price', 0)), 'inline': True}
                      ]
                  }
                  requests.post(webhook, json={'embeds': [embed]})
          "
      
      - name: Commit signals
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git fetch origin
          
          if git show-ref --verify --quiet refs/remotes/origin/data-storage; then
            git checkout data-storage
          else
            git checkout --orphan data-storage
            git rm -rf . 2>/dev/null || true
          fi
          
          git checkout main -- data/processed/ 2>/dev/null || true
          git checkout main -- signals/ 2>/dev/null || true
          
          git add data/processed/ signals/ 2>/dev/null || true
          
          if ! git diff --cached --quiet; then
            git commit -m "Signals - $(date -u) [${{ github.run_id }}]"
            git push "https://${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git" data-storage || true
          fi
          
          git checkout main || true

  # Job 3: Report status
  report-status:
    runs-on: ubuntu-latest
    needs: [fetch-data, calculate-indicators]
    if: always()
    
    steps:
      - name: Report status
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        run: |
          DATA="${{ needs.fetch-data.result }}"
          INDICATOR="${{ needs.calculate-indicators.result }}"
          
          if [ "$DATA" == "success" ] && [ "$INDICATOR" == "success" ]; then
            COLOR="3066993"
            TITLE="âœ… AEGIS Success"
          elif [ "$DATA" == "success" ]; then
            COLOR="16776960"
            TITLE="âš ï¸ AEGIS Partial"
          else
            COLOR="15158332"
            TITLE="âŒ AEGIS Failed"
          fi
          
          curl -H "Content-Type: application/json" \
            -d "{
              \"embeds\": [{
                \"title\": \"$TITLE\",
                \"color\": $COLOR,
                \"fields\": [
                  {\"name\": \"Data\", \"value\": \"$DATA\", \"inline\": true},
                  {\"name\": \"Signals\", \"value\": \"$INDICATOR\", \"inline\": true}
                ]
              }]
            }" \
            $DISCORD_WEBHOOK 2>/dev/null || echo "Discord failed"
